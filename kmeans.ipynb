{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read scraped data \n",
    "df_jobs = pd.read_csv('Indeed_data_small.csv')\n",
    "job_description = open('JD.txt').read().split('\\n BREAKS HERE')\n",
    "job_description = job_description[:-1]\n",
    "print(len(job_description))\n",
    "print(type(job_description[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing and Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load stopwords and stemmer function from NLTK library. Stop words are words like \"a\", \"the\", or \"in\" which don't convey significant meaning. Stemming is the process of breaking a word down into its root.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use nltk's English stopwords.\n",
    "english_stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "print(\"We use \" + str(len(english_stopwords)) + \" stop-words from nltk library.\")\n",
    "print(english_stopwords[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization and Lemmatization using stopwords\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def tokenization_and_lemmatization(text):\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent) if word not in english_stopwords]\n",
    "    filtered_tokens = []\n",
    "    # filtering out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    lemma = [wordnet_lemmatizer.lemmatize(t) for t in filtered_tokens]\n",
    "    return lemma\n",
    "\n",
    "\n",
    "def tokenization(text):\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent) if word not in english_stopwords]\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use our defined functions to analyze (i.e. tokenize, stem) our synopses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_lemmatize = []\n",
    "docs_tokenized = []\n",
    "for s in job_description:\n",
    "    s = s.encode()\n",
    "    s = s.decode('utf-8')\n",
    "    tokenized_and_stemmed_results = tokenization_and_lemmatization(s)\n",
    "    docs_lemmatize.extend(tokenized_and_stemmed_results)\n",
    "    tokenized_results = tokenization(s)\n",
    "    docs_tokenized.extend(tokenized_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_frame_dict = {docs_lemmatize[x]:docs_tokenized[x] for x in range(len(docs_lemmatize))}\n",
    "print(vocab_frame_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a mapping from stemmed words to original tokenized words for result interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define vectorizer parameters\n",
    "tfidf_model = TfidfVectorizer(max_df=0.8, max_features=200000,\n",
    "                                 min_df=0.2, stop_words='english',\n",
    "                                 use_idf=True, tokenizer=tokenization_and_lemmatization, ngram_range=(1,1))\n",
    "\n",
    "#fit the vectorizer to job description\n",
    "tfidf_matrix = tfidf_model.fit_transform(job_description)\n",
    "\n",
    "print(\"In total, there are \" + str(tfidf_matrix.shape[0]) + \\\n",
    "      \" job postings and \" + str(tfidf_matrix.shape[1]) + \" terms.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the terms identified by TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_selected_words = tfidf_model.get_feature_names_out()\n",
    "print(tf_selected_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting clusters\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_clusters = 4\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "km.fit(tfidf_matrix)\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(clusters))\n",
    "print(clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check K-means results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create DataFrame films from all the input files.\n",
    "np.array(clusters)  \n",
    "df_jobs['cluster'] = pd.Series(np.array(clusters) , index=df_jobs.index)\n",
    "df_jobs.head(6000)\n",
    "df_jobs.to_csv('kmeans_result.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert search to ints\n",
    "cleanup_nums = {\"Search\":     {'Data+scientist': 0, 'Machine+learning engineer': 1, 'Data+analyst': 2},}\n",
    "df_jobs.replace(cleanup_nums, inplace=True)\n",
    "df_jobs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of jobs included in each cluster:\")\n",
    "df_jobs['cluster'].value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# km.cluster_centers_ denotes the importance of each item in centroid.\n",
    "# need to sort it in descending order and get the top k items.\n",
    "\n",
    "print(\"<Document clustering result by K-means>\")\n",
    "\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "\n",
    "cluster_keywords_summary = {}\n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster \" + str(i) + \" words:\")\n",
    "    cluster_keywords_summary[i] = []\n",
    "    for words in order_centroids[i, :30]: # get the top 6 words of each cluster\n",
    "        cluster_keywords_summary[i].append(vocab_frame_dict[tf_selected_words[words]])\n",
    "        print(vocab_frame_dict[tf_selected_words[words]] + \", \")\n",
    "    print \n",
    "\n",
    "    cluster_jobs = df_jobs.loc[df_jobs.cluster == i, 'Title'].values.tolist()\n",
    "    print(\"Cluster \" + str(i) + \" titles (\" + str(len(cluster_jobs)) + \" jobs): \" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(order_centroids[1,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = decomposition.PCA(n_components=4)\n",
    "tfidf_matrix_np=tfidf_matrix.toarray()\n",
    "pca.fit(tfidf_matrix_np)\n",
    "X = pca.transform(tfidf_matrix_np)\n",
    "\n",
    "xs, ys = X[:, 0], X[:, 1]\n",
    "\n",
    "#set up colors per clusters using a dict\n",
    "cluster_colors = {0: 'g', 1: 'b', 2: 'r', 3: 'y', 4:'k',5:'m'}\n",
    "#set up cluster names using a dict\n",
    "cluster_names = {}\n",
    "for i in range(num_clusters):\n",
    "    cluster_names[i] = \", \".join(cluster_keywords_summary[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "#create data frame with PCA cluster results\n",
    "df = pd.DataFrame(dict(x=xs, y=ys, label=clusters)) \n",
    "groups = df.groupby(clusters)\n",
    "\n",
    "# set up plot\n",
    "fig, ax = plt.subplots(figsize=(16, 10))\n",
    "#Set color for each cluster/group\n",
    "for name, group in groups:\n",
    "    ax.plot(group.x, group.y, marker='o', linestyle='', ms=8, \n",
    "            label=cluster_names[name], color=cluster_colors[name], \n",
    "            mec='none')\n",
    "\n",
    "ax.legend(numpoints=1,loc=4)  #show legend with only 1 point, position is right bottom.\n",
    "\n",
    "plt.show() #show the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "search_names = ['Machine+learning engineer', 'Data+scientist', 'Data+analyst']\n",
    "#create data frame with PCA cluster results\n",
    "search_num = df_jobs['Search'].tolist()\n",
    "df_indeed = pd.DataFrame(dict(x=xs, y=ys, label=search_num)) \n",
    "groups2 = df_indeed.groupby(search_num)\n",
    "\n",
    "# set up plot\n",
    "cluster_colors2 = {0: 'g', 1: 'b', 2: 'r', 3: 'y', 4:'k',5:'m'}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 10))\n",
    "#Set color for each cluster/group\n",
    "for name, group in groups2:\n",
    "    ax.plot(group.x, group.y, marker='o', linestyle='', ms=8, \n",
    "            label=search_names[name], color=cluster_colors2[name], \n",
    "            mec='none')\n",
    "\n",
    "ax.legend(numpoints=1,loc=4)  #show legend with only 1 point, position is right bottom.\n",
    "\n",
    "plt.show() #show the plot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
